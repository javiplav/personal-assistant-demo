# Demo Configuration Guide

The Personal Assistant Demo supports multiple LLM configurations. This guide explains how to run the demo with different setups.

## ğŸš€ Quick Start

### Default (Ollama - Recommended for Local Development)
```bash
# Run with default Ollama configuration
python demo/demo_showcase.py

# Or explicitly specify
python demo/demo_showcase.py --config configs/config-ollama.yml
```

### NIM (Cloud-based - Recommended for Production)
```bash
# Set your NVIDIA API key first
export NVIDIA_API_KEY="your_api_key_here"

# Run with NIM configuration
python demo/demo_showcase.py --config configs/config.yml
```

### Ollama with Environment Variables
```bash
# Optional: Set custom Ollama settings
export OLLAMA_BASE_URL="http://localhost:11434/v1"
export OLLAMA_MODEL="qwen2.5:7b"

# Run with environment-based config
python demo/demo_showcase.py --config configs/config-ollama-env.yml
```

## ğŸ“‹ Configuration Details

### 1. `configs/config-ollama.yml` (Default)
- **LLM Backend**: Ollama (Local)
- **Model**: qwen2.5:7b
- **Requirements**: 
  - Ollama running locally
  - qwen2.5:7b model downloaded (`ollama pull qwen2.5:7b`)
- **Benefits**: 
  - âœ… Privacy (fully local)
  - âœ… No API costs
  - âœ… Fast response times
  - âœ… Works offline

### 2. `configs/config.yml` (NIM)
- **LLM Backend**: NVIDIA NIM
- **Model**: nvidia/llama-3.1-nemotron-70b-instruct
- **Requirements**: 
  - Valid NVIDIA API key
  - Internet connection
- **Benefits**: 
  - âœ… Enterprise-grade model (70B parameters)
  - âœ… Optimized for tool calling
  - âœ… No local compute requirements
  - âœ… Production-ready scalability

### 3. `configs/config-ollama-env.yml` (Flexible Ollama)
- **LLM Backend**: Ollama (Local with Environment Variables)
- **Model**: Configurable via `OLLAMA_MODEL`
- **Requirements**: 
  - Ollama running locally
  - Model specified in environment
- **Benefits**: 
  - âœ… Flexible model selection
  - âœ… Environment-based configuration
  - âœ… Easy deployment variations

## ğŸ› ï¸ Setup Instructions

### For Ollama Configurations

1. **Install Ollama**
   ```bash
   # macOS
   brew install ollama
   
   # Or download from https://ollama.ai
   ```

2. **Start Ollama Service**
   ```bash
   ollama serve
   ```

3. **Download Required Model**
   ```bash
   ollama pull qwen2.5:7b
   ```

4. **Verify Installation**
   ```bash
   ollama list  # Should show qwen2.5:7b
   ```

### For NIM Configuration

1. **Get NVIDIA API Key**
   - Visit [NVIDIA Developer Portal](https://developer.nvidia.com/)
   - Create account and generate API key

2. **Set Environment Variable**
   ```bash
   # Add to your ~/.zshrc or ~/.bashrc
   export NVIDIA_API_KEY="your_api_key_here"
   
   # Or set temporarily
   export NVIDIA_API_KEY="your_api_key_here"
   ```

3. **Test API Access**
   ```bash
   curl -H "Authorization: Bearer $NVIDIA_API_KEY" \
        https://integrate.api.nvidia.com/v1/models
   ```

## ğŸ”§ Troubleshooting

### Ollama Issues

**Problem**: `Ollama is not running`
```bash
# Solution: Start Ollama service
ollama serve
```

**Problem**: `qwen2.5 model not found`
```bash
# Solution: Download the model
ollama pull qwen2.5:7b
```

**Problem**: `Connection refused to localhost:11434`
```bash
# Check if Ollama is running
ps aux | grep ollama

# Restart Ollama
ollama serve
```

### NIM Issues

**Problem**: `[401] Unauthorized`
```bash
# Solution: Set valid API key
export NVIDIA_API_KEY="your_valid_key"
```

**Problem**: `API key is required for the hosted NIM`
```bash
# Solution: Check environment variable
echo $NVIDIA_API_KEY

# Set if missing
export NVIDIA_API_KEY="your_api_key"
```

### General Issues

**Problem**: `Command failed with return code 1`
```bash
# Check NAT installation
nat --version

# Reinstall if needed
pip install nemo-agent-toolkit
```

**Problem**: `find_client_by_name not found`
```bash
# Ensure you're using updated configurations
# All configs should include find_client_by_name in functions and tool_names
```

## ğŸ¯ Choosing the Right Configuration

### Use **Ollama** (`config-ollama.yml`) when:
- ğŸ  Developing locally
- ğŸ”’ Privacy is important
- ğŸ’° Want to avoid API costs
- ğŸŒ Working offline
- ğŸ”§ Learning and experimentation

### Use **NIM** (`config.yml`) when:
- ğŸ¢ Production deployment
- ğŸ’ª Need best model performance
- â˜ï¸ Cloud-based infrastructure
- ğŸ“ˆ Scalability requirements
- ğŸš€ Enterprise features needed

### Use **Ollama + Env** (`config-ollama-env.yml`) when:
- ğŸ”„ Multiple deployment environments
- ğŸ›ï¸ Need configuration flexibility
- ğŸ“¦ Container-based deployment
- ğŸ”§ CI/CD pipeline integration

## ğŸ“Š Performance Comparison

| Configuration | Response Time | Model Size | Setup Complexity | Cost |
|---------------|---------------|------------|------------------|------|
| Ollama        | ~8-12s        | 7B params  | Medium          | Free |
| NIM           | ~3-8s         | 70B params | Low             | Pay-per-use |
| Ollama + Env  | ~8-12s        | Variable   | Medium          | Free |

## ğŸ‰ Success Indicators

When everything is working correctly, you should see:

```
ğŸš€ NeMo Agent Toolkit: Enterprise Solutions Architect Demo
=================================================================
ğŸ“‹ Configuration: [Your Config Type]
ğŸ”§ Config File: [Your Config File]
=================================================================

ğŸ” Validating system requirements...

1ï¸âƒ£ Checking [LLM Service]...
   âœ… [Service] is running with model: [model_name]
2ï¸âƒ£ Checking NAT installation...
   âœ… NAT is working correctly
3ï¸âƒ£ Configuration validated
   âœ… Test result: [Calculation result]

ğŸ‰ All systems ready! Starting enterprise demo...
```

## ğŸ“ Support

If you encounter issues:

1. Check this troubleshooting guide
2. Verify your configuration matches the examples
3. Test with a simple calculation first
4. Check logs for specific error messages
5. Ensure all dependencies are installed

For more help, refer to the main [README.md](../README.md) or NVIDIA NeMo Agent Toolkit documentation.
